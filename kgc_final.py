# -*- coding: utf-8 -*-
"""KGC_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQ-0-MED_2eXt2NpA-_MNu3qBy3N9ahH
"""


import os
import stanza
from indicnlp.tokenize import indic_tokenize
import re

stanza.download("te")
telugu_nlp = stanza.Pipeline("te", processors="tokenize,pos,lemma,depparse")

'''from google.colab import drive
drive.mount('/content/drive')'''

def tokenize_telugu(text):
    return indic_tokenize.trivial_tokenize(text)

'''import os
import csv

all_triplets = []
telugu_stop_words = set([
    'అందరు', 'అప్పుడు', 'అయితే', 'ఇప్పటికే', 'అందులో', 'అందుకే', 'ఇంకా', 'ఇప్పటికీ', 'కానీ', 'కేవలం', 'గురించి', 'జరిగిన', 'ఇచ్చిన', 'ఎవరూ', 'ఇక్కడ', 'అందువల్ల',
    'అలాగే', 'ఆయన', 'ఎందుకంటే', 'ఎందుకు', 'ఎక్కడ', 'కూడా', 'తో పాటు', 'నుండి', 'తర్వాత', 'తర్వాతే', 'తీసుకుని', 'తన', 'తమ', 'తమకు', 'తనకు', 'లేదా',
    'వీరు', 'వీటిని', 'వారు', 'వీళ్ళు', 'వీరే', 'ఇతర', 'వాళ్ళు', 'అప్పటి', 'మరియు', 'ఇంత', 'తాము', 'అది', 'అంటే', 'అన్నా', 'ఇవ్వడం', 'కాదు', 'వాడు', 'అయినా',
    'అలా', 'మరి', 'కాని', 'ఎప్పుడూ', 'కనుక', 'ఇచ్చి', 'అప్పట్లో', 'అవును', 'అప్పుడప్పుడు', 'ఇది', 'ఉంటే', 'కూడా', 'ఎందుకంటే', 'కాని', 'ఎక్కువ', 'మాత్రం',
    'వంటి', 'ఉన్న', 'ఉంది', 'వచ్చిన', 'కి', 'మీ'
])files = /content/drive/My Drive/Dataset_N190457

def extracting_triplets():
    # Specify the maximum number of files to process
    max_files = 1000
    # Counter to keep track of the number of files processed
    processed_count = 0

    for filename in os.listdir(files):
        if filename.endswith(".txt"):
            file_path = os.path.join(files, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                article_text = file.read()

            # Tokenize Telugu text
            telugu_tokens = tokenize_telugu(article_text)

            # Process Telugu text using Stanza
            doc = telugu_nlp(' '.join(telugu_tokens))
            for word in sent.words:
                    if sc <= 1 and rc <= 0 and (word.deprel == 'nsubj' or word.upos in ['NOUN', 'PROPN']):  # Subject
                        if word.text not in telugu_stop_words:
                            sub += word.text + " "
                            sc += 1
                    elif word.deprel in ['obj', 'dobj', 'iobj', 'pobj', 'obl']:  # Object
                        if word.text not in telugu_stop_words:
                            obj += word.text + " "
                            oc += 1
                    elif word.deprel == 'root' or word.upos in ['AUX']:#ADV', 'ADP']:  # Relation
                        if word.text not in telugu_stop_words:
                            rel += word.text + " "
                            rc += 1

                # Strip trailing spaces and append entities and relations to the all_triplets list
                if sub.strip() and obj.strip() and rel.strip():
                    all_triplets.append([sub.strip(), rel.strip(), obj.strip()])

            processed_count += 1
            if processed_count >= max_files:
                break


    return all_triplets

# Call the function to process the files
triplets = extracting_triplets()
# Define the CSV file path
csv_file_path = '/content/drive/My Drive/triplets190457.csv'

# Write the output to a CSV file
with open(csv_file_path, mode='w', encoding='utf-8', newline='') as csv_file:
    writer = csv.writer(csv_file)
    # Write the header
    writer.writerow(['Subject', 'Relation', 'Object'])
    # Write the triplets
    for triplet in triplets:
        writer.writerow(triplet)

print(f"Triplets have been written to {csv_file_path}")'''

import csv

file_path =  "C:\\Users\\chakr\\Downloads\\triplets190457.csv"

def read_triplets(file_path):
    triplets = []
    with open(file_path, 'r', encoding='utf-8') as file:
        reader = csv.reader(file)
        next(reader)
        for row in reader:
            if len(row) == 3:
                triplets.append(tuple(row))
    return triplets

triplets = read_triplets(file_path)

type(triplets[0])

len(triplets)

triplets

import torch
import torch.nn as nn
import torch.optim as optim

entities = set()
relations = set()

for head, relation, tail in triplets:
    entities.add(head)
    entities.add(tail)
    relations.add(relation)

entity2idx = {entity: idx for idx, entity in enumerate(entities)}
relation2idx = {relation: idx for idx, relation in enumerate(relations)}

indexed_triplets = [(entity2idx[head], relation2idx[relation], entity2idx[tail]) for head, relation, tail in triplets]

thead = torch.LongTensor([triplet[0] for triplet in indexed_triplets])
trelation = torch.LongTensor([triplet[1] for triplet in indexed_triplets])
ttail = torch.LongTensor([triplet[2] for triplet in indexed_triplets])

entity2idx.values()

indexed_triplets

import random

import random

def generate_negative_triplets(true_triplets, entities):
    negative_triplets = []
    entity_list = list(entities)  # Convert set to list

    if not entity_list:  # Check if the entity_list is empty
        raise ValueError("The entity list is empty.")

    for triplet in true_triplets:
        head, relation, tail = triplet

        negative_head = random.choice(entity_list)
        while negative_head == head:
            negative_head = random.choice(entity_list)

        negative_tail = random.choice(entity_list)
        while negative_tail == tail:
            negative_tail = random.choice(entity_list)

        negative_triplets.append([negative_head, relation, negative_tail])

    return negative_triplets

negative_triplets = generate_negative_triplets(triplets, entities)

neg_entities = set()
for head, relation, tail in negative_triplets:
    neg_entities.add(head)
    neg_entities.add(tail)


neg_indexed_triplets = [(entity2idx[head], relation2idx[relation], entity2idx[tail]) for head, relation, tail in negative_triplets]

neg_head = torch.LongTensor([triplet[0] for triplet in neg_indexed_triplets])
neg_relation = torch.LongTensor([triplet[1] for triplet in neg_indexed_triplets])
neg_tail = torch.LongTensor([triplet[2] for triplet in neg_indexed_triplets])

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class TransEModel(nn.Module):
    def __init__(self, num_entities, num_relations, embedding_dim, margin):
        super(TransEModel, self).__init__()
        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)
        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)
        self.margin = margin
        self.loss_fn = nn.MarginRankingLoss(margin=margin)

        # Initialize embeddings
        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)
        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)

    def forward(self, head, relation, tail, neg_head, neg_tail):
        head_emb = self.entity_embeddings(head)
        relation_emb = self.relation_embeddings(relation)
        tail_emb = self.entity_embeddings(tail)
        neg_head_emb = self.entity_embeddings(neg_head)
        neg_tail_emb = self.entity_embeddings(neg_tail)

        pos_dist = torch.norm(head_emb + relation_emb - tail_emb, p=1, dim=1)
        neg_dist_head = torch.norm(neg_head_emb + relation_emb - tail_emb, p=1, dim=1)
        neg_dist_tail = torch.norm(head_emb + relation_emb - neg_tail_emb, p=1, dim=1)

        return pos_dist, neg_dist_head, neg_dist_tail

    def score_triplets(self, head, relation, tail):
        head_emb = self.entity_embeddings(head)
        relation_emb = self.relation_embeddings(relation)
        tail_emb = self.entity_embeddings(tail)
        dist = torch.norm(head_emb + relation_emb - tail_emb, p=1, dim=1)
        return dist

    def loss(self, pos_dist, neg_dist_head, neg_dist_tail):
        target = torch.ones(pos_dist.size())
        return self.loss_fn(pos_dist, neg_dist_head, target) + self.loss_fn(pos_dist, neg_dist_tail, target)

# Hyperparameters
num_entities = len(entity2idx)
num_relations = len(relation2idx)
embedding_dim = 64
margin = 1.0
learning_rate = 0.001
num_epochs = 100

# Create the model
model = TransEModel(num_entities, num_relations, embedding_dim, margin)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    pos_dist, neg_dist_head, neg_dist_tail = model(thead, trelation, ttail, neg_head, neg_tail)
    loss = model.loss(pos_dist, neg_dist_head, neg_dist_tail)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")



entity_embeddings = model.entity_embeddings.weight.data.cpu().numpy()
relation_embeddings = model.relation_embeddings.weight.data.cpu().numpy()

# Create embedding dictionaries
true_embeddings_dict = {idx : entity_embeddings[idx] for entity, idx in entity2idx.items()}
true_embeddings_dict.update({idx : relation_embeddings[idx] for relation, idx in relation2idx.items()})

neg_embeddings_dict = true_embeddings_dict.copy()

true_embeddings_dict

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def calculate_cosine_similarities(triples, embeddings_dict):
    similarities = []
    for head, relation, obj in triples:
        if head in embeddings_dict and relation in embeddings_dict and obj in embeddings_dict:
            head_embedding = embeddings_dict[head]
            relation_embedding = embeddings_dict[relation]
            obj_embedding = embeddings_dict[obj]

            # Calculate cosine similarities
            head_obj_similarity = cosine_similarity([head_embedding], [obj_embedding])[0][0]
            head_relation_similarity = cosine_similarity([head_embedding], [relation_embedding])[0][0]
            relation_obj_similarity = cosine_similarity([relation_embedding], [obj_embedding])[0][0]

            # Average cosine similarity
            average_similarity = (head_obj_similarity + head_relation_similarity + relation_obj_similarity) / 3

            similarities.append((head, relation, obj, average_similarity))
        else:
            similarities.append((head, relation, obj, None))  # In case of missing embeddings
    return similarities

def compare_triplets(true_triplets, negative_triplets, true_embeddings_dict, negative_embeddings_dict, threshold=0):
    true_similarities = calculate_cosine_similarities(true_triplets, true_embeddings_dict)
    negative_similarities = calculate_cosine_similarities(negative_triplets, negative_embeddings_dict)

    valid_negative_triplets = []

    for true_triplet, neg_triplet in zip(true_similarities, negative_similarities):
        true_head, true_relation, true_obj, true_similarity = true_triplet
        neg_head, neg_relation, neg_obj, neg_similarity = neg_triplet

        if true_similarity is not None and neg_similarity is not None and true_similarity > neg_similarity + threshold:
            valid_negative_triplets.append(neg_triplet[:3])

    return valid_negative_triplets

# Example usage to identify hidden triplets from negative samples
hidden_rels = compare_triplets(indexed_triplets, neg_indexed_triplets, true_embeddings_dict, neg_embeddings_dict, threshold = 0.2)
hidden_relations = list(set(hidden_rels))
print("Valid negative triplets", hidden_relations)

valid_neg_tuples = []
entity_keys = list(entity2idx.keys())
entity_values = list(entity2idx.values())
relation_keys = list(relation2idx.keys())
relation_values = list(relation2idx.values())

for triplet in hidden_relations:
    head_entity = entity_keys[entity_values.index(triplet[0])]
    relation = relation_keys[relation_values.index(triplet[1])]
    tail_entity = entity_keys[entity_values.index(triplet[2])]
    valid_neg_tuples.append((head_entity, relation, tail_entity))

# Print valid_neg_tuples to verify the result
print(valid_neg_tuples)

def score_triple(triple, embeddings):
    head, relation, tail = triple
    head_emb = embeddings[head]
    relation_emb = embeddings[relation]
    tail_emb = embeddings[tail]

    score = np.linalg.norm(head_emb + relation_emb - tail_emb)
    return -score

def hits_at_k(true_triples, false_triples, embeddings, ks):
    true_ranks = []
    true_scores = [score_triple(triple, embeddings) for triple in true_triples]
    false_scores = [score_triple(triple, embeddings) for triple in false_triples]

    hits_at_ks = {k: 0 for k in ks}
    for true_score in true_scores:
        rank = sum(1 for false_score in false_scores if false_score < true_score) + 1
        true_ranks.append(rank)
        for k in ks:
            if rank <= k:
                hits_at_ks[k] += 1

    mean_rank = np.mean(true_ranks)
    mrr = np.mean([1.0 / rank for rank in true_ranks])
    hits_at_k_scores = {k: hits / len(true_triples) for k, hits in hits_at_ks.items()}

    return hits_at_k_scores, mean_rank, mrr



hits_at_k_scores, mean_rank, mrr = hits_at_k(indexed_triplets,neg_indexed_triplets, true_embeddings_dict, [1, 5, 10])
print(f"Hits@1: {hits_at_k_scores[1]}")
print(f"Hits@5: {hits_at_k_scores[5]}")
print(f"Hits@10: {hits_at_k_scores[10]}")
print(f"Mean Rank: {mean_rank}")
print(f"Mean Reciprocal Rank (MRR): {mrr}")



